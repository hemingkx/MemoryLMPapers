# Memory-enhanced-LM-Papers

## Papers

### Language Modeling

- [17ICLR] [Improving Neural Language Models with a Continuous Cache][https://arxiv.org/abs/1612.04426]

- [20ICLR] [Generalization through Memorization: Nearest Neighbor Language Models][https://arxiv.org/abs/1911.00172]
- [22ACL] [Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data][https://arxiv.org/abs/2203.08773]
- [22ICLR] [Memorizing Transformers][https://arxiv.org/abs/2203.08913]
- [22Arxiv] [Training Language Models with Memory Augmentation][https://arxiv.org/abs/2205.12674]

### Efficiency

- [21EMNLP] [Efficient Nearest Neighbor Language Models][https://arxiv.org/abs/2109.04212]

### Long-range  Sequence Modeling

- [19ACL] [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context][https://arxiv.org/abs/1901.02860]
- [19Arxiv] [Compressive Transformers for Long-Range Sequence Modelling][https://arxiv.org/abs/1911.05507]
- [22ACL] [$\infty$-former: Infinite Memory Transformer][https://arxiv.org/abs/2109.00301]
- [22NAACL] [LaMemo: Language Modeling with Look-Ahead Memory][https://arxiv.org/abs/2204.07341]

